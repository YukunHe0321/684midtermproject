---
title: "684 Midterm Project"
author: "Yukun He"
date: "2017/12/19"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(dev = 'png')
pdf.options(useDingbats = TRUE)
```

```{r}
pacman::p_load(
data.table,
lubridate,
ggthemes,
grid,
gridExtra,
scales,
ggplot2,
dygraphs,
RColorBrewer,
tidyr,
rmarkdown,
lme4,
arm,
merTools,
wordcloud
)
```
###===Intro===

For this project, I want to focus on the video game datasets. I am a huge fan of all kinds of video games, such that I would like to devote myself to the video game industry in the future. I think this project provides a good chance for me to start my analyze on video games related datasets. Video games are generated by computer programming and are based on large datasets to keep the pseudorandomness of themselves. Also, players' own unique play style can come up with interesting statistics for us to analyze. Before choosing this CS:GO dataset, I also tried to explore several different datasets in the field of video game. Throughout the exploration of different dataset, I think this one can be interesting to interpret. 

Counter-Strike: Global Offensive (CS:GO) is a multiplayer first-person shooter (FPS) video game on the PC platform. As an online real-time competitive game, CS:GO generates numerous data in many aspects, result from a large amount of players' unique gaming styles. Firstly, lets take a look at the components of this dataset:

- MAP: Name of the map
- Team: Randomly generated team ids, players on the same team have the same team id
- Player: Randomly generated player ids, the same player will have the same player id
- Kills: Number of kills that a particular player score in that game
- Deaths: Number of deaths that a particular player is killed in that game
- ADR: Average damage per round
- KAST.: Percentage of rounds in which the player either had a kill, assist, survived or was traded
- Rating: The overall evaluation of a player based on his/her performance in that game
- Rating.Type : Types of rating players
- MatchID: Randomly generated match ids, players in the same match will have the same match id

From the dataset, we can see that the Rating column provides overall evaluations of players in different games. The higher the rating, the better the performance. In this situation, I think it can be interesting to explore the elements that bring up a well-performed player, based on the different groups of Maps each game takes place in. Since different maps have completely different gaming tactics and landscapes, I think maps will have huge effect on players' performance. Each player will have maps that he/she is good at and bad at.  Professional CS:GO teams can use this analysis to change their training focus of pro players and their map choosing strategies in professional matches. And other players on the internet can find out the aspects they need to improve in the future gaming, in order to get better rating. 

###===Data Cleaning (Concerns)===

During data cleaning, I think the team id,  player id, and match id columns are irrelavent to my analysis, since my focus is on the Rating. Hence I would consider players with same player ids in different matches as different players. Also, I find out that when the Rating Type is 1, the ADR and KAST. columns are all N/As. If I choose to omit the N/As in the dataset, the groups of Maps will be decreased from 14 to 9, which does not satisfy the requirement of minimum groups for this project. In this situation, I have to exclude those two columns from my modeling, and explore the effect from number of kills and deaths. I think Kills and Deaths are two major effects, so that the modeling can still be convincing. However, without other possible elements, this modeling lacks some fun of exploration. But I still believe with current cleaned dataset, I can come up with some useful models. 

```{r}

data_csgo <- read.csv("playerStats.csv")
#data_csgo <- na.omit(data_csgo)
table(data_csgo$Map)

data_csgo <- select(data_csgo, Map, Kills, Deaths, Rating)
data_csgo$Player_ID <- c(1:length(data_csgo$Map))

```

###===EDAs===

In this section, I want to present an overview of the dataset I choose via EDAs. Also, I expect to find additional interesting elements that I can mention and explore in my modeling. 

##PART 1

For the first part of EDAs, we can see the number of occurence of each map in our dataset. There are 9 leading maps that people plays the most, and 5 maps people relatively seldomly play. From the wordcloud we can more directly see that the bolder and bigger words are the maps players choose more. Here comes a question that I want to include in my models: Does higher occurrence of maps represents higher proficiency of players in these maps, which results in higher rating? 

```{r}

data_csgo_eda1 <- select(data_csgo, Player_ID, Map)

data_csgo_eda1 <- melt(data_csgo_eda1, id.vars="Player_ID")
data_csgo_eda1 <- select(data_csgo_eda1, Player_ID, value)

data_csgo_eda1 <- data_csgo_eda1 %>% 
  count(value, sort = TRUE)

ggplot(data_csgo_eda1, aes(reorder(value,n),n,fill=value)) + 
  geom_col() + 
  coord_flip() + 
  xlab("Number of occurrence") +
  ylab("Name of Maps")

data_csgo_eda1 %>%
  with(wordcloud(value, n))

```

##Part 2

In the second part of the EDAs. I seek to use boxplot of rating based on different maps to figure out the mean values of rating for those popular maps. However, the difference of each map does not vary too much. I think regression is needed to distinguish the subtle differences.

```{r}

data_csgo_eda2 <- select(data_csgo, Map, Rating)

ggplot(data_csgo_eda2,aes(x=Map,y=Rating,fill=Map)) +
  geom_boxplot() + 
  stat_summary(geom = "point", color = "yellow",
            fun.y = "mean", size = 1, position=position_dodge(width=0.7)) +
  coord_flip()

```

##Part 3

For the third part of EDAs, we can see the boxplots for Kills and Deaths, grouped by Maps. We observe a large number of outliers. I think this is mainly because of the existence of elite players and rookie players, which results in a huge difference of the max and min number of Kills and Deaths. Rookie players tend to die more and kill less, while elite players tend to die less and kill more. From the first boxplot of this part, we can see the yellow dots representing average kills is slightly lower than the average deaths in every map. And when we compare the kills and deaths separately among maps, the kills and deaths each does not vary too much. Does this means that number of kills and number of deaths have exactly opposite effect on the rating? We should try to solve the question in the modeling. 

```{r}
data_csgo_eda3 <- select(data_csgo, Map,Kills,Deaths)
data_csgo_eda3 <- melt(data_csgo_eda3, id.vars = "Map")

ggplot(data_csgo_eda3,aes(x=Map,y=value,fill=variable)) +
  geom_boxplot() +
  stat_summary(geom = "point", aes(group=variable), color = "yellow",
            fun.y = "mean", size = 1, position=position_dodge(width=0.7)) +
  coord_flip() +
  xlab("Number of Kills / Deaths")

ggplot(data_csgo_eda3,aes(x=Map,y=value,fill=variable)) +
  geom_boxplot() +
  stat_summary(geom = "point", aes(group=Map), color = "yellow",
            fun.y = "mean", size = 1, position=position_dodge(width=0.7)) +
  coord_flip() +
  xlab("Number of Kills / Deaths") +
  facet_wrap(~variable, scale = "free")

```

###===Fixed Effects===

In this part, I fitted 4 models with fixed effects, by adding group factors and interactions. The R-square for each model does not differ too much, however, considering a large dataset we have, I think R-square cannot be used to judge the models. Furthermore, I compare the AIC values for each model, and I find out that model 3 and model 4 have lower AIC values. While model 3 consists of less variables, I think model 3 is a better one.

Let's interpret the coefficients of model 3:
- For intercept, since Deaths and Kills cannot be 0 and there must be one map selected, we do not interpret the intercept alone.
- For the coefficient of kills, every one kill for the player in one match will increase 0.05860 points to the basic rating 0.7136.
- For the coefficient of deaths, every one death for the player in one match will decrease 0.03216 points to the basic rating 0.7136.
- For the coefficient of interaction between kills and deaths, every one kill or death for the player in one match will decrease the effect on basic rating 0.7136 from deaths or kills by 0.0006134.
- For the coefficients of the group of maps, different maps will increase the rating by 0.05688, 0.06984, 0.05706, 0.005472, 0.02421, 0.03454, 0.01150, 0.04363, 0.03133, 0.06766, 0.0002677, and 0.05081, except the map Tuscan, which decreases the rating by 0.0009764. This answer the question in EDA part. The popular maps does not warrant higher rating.

As we can see, the coefficients are all quite small, but the mean values for kills and deaths are both around 20. Hence the effect on overall rating for a player is still relatively large.

```{r}

csgo_fixed_reg1 <- lm(Rating ~ Kills + Deaths, data_csgo)
csgo_fixed_reg2 <- lm(Rating ~ Kills + Deaths + (Kills*Deaths), data_csgo)
csgo_fixed_reg3 <- lm(Rating ~ Kills + Deaths + (Kills*Deaths) + factor(Map), data_csgo)
csgo_fixed_reg4 <- lm(Rating ~ Kills + Deaths + (Kills*Deaths) + factor(Map) + (Kills*Map + Deaths*Map), data_csgo)

```

```{r}

summary(csgo_fixed_reg1)$r.squared
summary(csgo_fixed_reg2)$r.squared
summary(csgo_fixed_reg3)$r.squared
summary(csgo_fixed_reg4)$r.squared

AIC(csgo_fixed_reg1,csgo_fixed_reg2,csgo_fixed_reg3,csgo_fixed_reg4)

summary(csgo_fixed_reg3)

```

###===Random Effects===

In this part, I explored and established several multilevel models with group of Maps. Furthermore, I plot the residual plots.  By comparing AIC and range of binned residual plot for each model, I choose one better fitted model. I also plot the simulated fixed effects and the simulated random effects on a ggplot2 chart for the model I choose.

##Model 1

```{r}

csgo_rand_reg1 <- lmer(Rating ~ Kills + (1 | Map), data_csgo)
summary(csgo_rand_reg1)

```

Firstly, I fit a model to use fixed effect Kills (number of kills) to predict Rating (overall rating), controlling for by-map variability. From the summary we can see the variance of Map, and the variance of  the Residual which stands for the variability that’s not due to Map. This means that there are still some more influential factors outside of our dataset other than Map which will effect the overall rating. Since our dataset do not contain those factors, we do not want to further explore this aspect. 

For the fixed effect part, we can see every one increase in Kills will result in 0.04089 increase in Rating.

##Model 2

```{r}

csgo_rand_reg2 <- lmer(Rating ~ Kills + Deaths + (1 | Map), data_csgo)
summary(csgo_rand_reg2)

```

In Model 2, I add Deaths as the additional fixed effect. Compare to the first model I fitted, the variation that's associated with Map dropped considerably. This is because the variation that’s due to number of deaths was confounded with the variation that’s due to maps. The Residual variance drops either. By adding Deaths, I have changed a large amount of the variance that was previously in the random effects component to the fixed effects component.

For the fixed effect part, we can see the intercept is significantly higher than that from model 1. As now we take number of deaths into consideration, the effect from the number of kills is enhanced, but not too much. Now the effects of Kills and Deaths have almost the opposite values. 

```{r}

coef(csgo_rand_reg1)
coef(csgo_rand_reg2)

```

Also, when we take a look at the group coefficients of two models, we can see the intercept for each map is increased considerably. The effect from the maps is stronger when we consider the number of deaths as a fixed effect.

##Model 3 and 4

```{r, warning=FALSE}

csgo_rand_reg3 <- lmer(Rating ~ Kills + Deaths + (1 + Kills | Map), data_csgo)
csgo_rand_reg4 <- lmer(Rating ~ Kills + Deaths + (1 + Deaths | Map), data_csgo)

coef(csgo_rand_reg3)
coef(csgo_rand_reg4)

```

These two random slope models take a long time to run. As we can see from the coefficients of the model 3 and model 4, the slope for Kills and Deaths  are different for each different map we choose. 

The slopes for Kills in model 3 are always positive and that many of the values are quite similar to each other. This means that despite individual variation, there is also consistency in how number of kills affects the overall rating. For all of the players, the rating tends to increase when score more number of kills in the game, but for some players it goes up slightly more.

The slopes for Deaths in model 4 are always negative and that many of the values are quite similar to each other. This means that despite individual variation, there is also consistency in how number of deaths affects the overall rating. For all of the players, the rating tends to decrease when score more number of deaths in the game, but for some players it goes down slightly more.

```{r}

AIC(csgo_rand_reg1, csgo_rand_reg2, csgo_rand_reg3, csgo_rand_reg4)

g1 <- ggplot(data_csgo, aes(fitted.values(csgo_rand_reg1), resid(csgo_rand_reg1))) + geom_point()
g1 <- g1 + ylab("residuals") + xlab("fitted value") + labs(title = "Residual Plot of Model 1")

g2 <- ggplot(data_csgo, aes(fitted.values(csgo_rand_reg2), resid(csgo_rand_reg2))) + geom_point()
g2 <- g2 + ylab("residuals") + xlab("fitted value") + labs(title = "Residual Plot of Model 2")

g3 <- ggplot(data_csgo, aes(fitted.values(csgo_rand_reg3), resid(csgo_rand_reg3))) + geom_point()
g3 <- g3 + ylab("residuals") + xlab("fitted value") + labs(title = "Residual Plot of Model 3")

g4 <- ggplot(data_csgo, aes(fitted.values(csgo_rand_reg4), resid(csgo_rand_reg4))) + geom_point()
g4 <- g4 + ylab("residuals") + xlab("fitted value") + labs(title = "Residual Plot of Model 4")

grid.arrange(g1, g2, g3, g4, ncol = 2, nrow = 2)

```

Then I generate the AIC for the 4 models in Random Effect part, also with their residual plots. From the AIC, model 2 and 4 have similar lower AIC but model 2 has fewer variables. Then we compare the residual plots. Both plots are balanced and with similar range of distribution. Hence, in the random effect part, model 2 is better.

###===Conclusion===

Through out the coding and analysis, I think I have explored some aspects and interesting facts about this dataset about CS:GO. The questions I mentioned at the beginning are also solved through the process. However, there are still improvements to be made in the future. Firstly, I failed to find a proper way to deal with the N/As in the dataset, such that I lost two columns of data. I could have done more interesting exploration with the help of those two columns. Furthermore, I only used AIC and residual plots to compare the effectiveness of different models. I noticed there are more ways to compare models, like: BIC and cross-calidation. I think in this winter break I should try to master more methods to compare models.

For the limitation, I think there is still something I need to improve in my models. As we can see, the residual plots for my random effect models are not pretty good, with skewness. I tried to take log to relieve this issue but failed. Due to the time limit of this project, I cannot perform further improvement of my work. But I will keep working with the improvement of my models.